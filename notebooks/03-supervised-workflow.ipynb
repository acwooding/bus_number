{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import Dataset, available_datasets\n",
    "from src import workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = workflow.available_datasets()\n",
    "dataset_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall task: \n",
    "\n",
    "Train a supervised model on the lvq-pak. Try three different techniques, and pick the one with the best accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = Dataset.load('lvq-pak_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = Dataset.load('lvq-pak_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_train.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype the workflow in a notebook!\n",
    "\n",
    "Let's start with one algorithm!\n",
    "\n",
    "`make train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(ds_train.data, ds_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = LinearSVC(random_state=42, max_iter=200000)\n",
    "model.fit(ds_train.data, ds_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use it to predict\n",
    "`make predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = model.predict(ds_test.data);\n",
    "p_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the quality of the prediction\n",
    "`make analysis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(ds_test.data, ds_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ds_test.target, p_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: \n",
    "* automate the basic workflow\n",
    "* compare 3 different algorithms for our Swedish Chef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: `make train`\n",
    "\n",
    "## Add our algorithm to available_algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are currently no available algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_algorithms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add an algorithm, add  a key:value pair to the dict `_ALGORITHMS` in `src/models/algorithms.py`.\n",
    "\n",
    "For example, add\n",
    "```\n",
    "'linearSVC': LinearSVC()\n",
    "```\n",
    "to the `_ALGORITHMS` dict, and add\n",
    "```\n",
    "from sklearn.svm import LinearSVC\n",
    "```\n",
    "to the top of the file.\n",
    "\n",
    "Also, add `linearSVC` to the docstring of `available_algorithms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_algorithms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(workflow.available_algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're in a position where the `make train` script can run using `linearSVC`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make -n train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that `make train` takes a `models/model_list.json` as input.\n",
    "```\n",
    "## train / fit / build models\n",
    "train: models/model_list.json\n",
    "\t$(PYTHON_INTERPRETER) -m src.models.train_model model_list.json\n",
    "```\n",
    "\n",
    "Under the hood, a `model_list.json` is a list of dicts, where each dict specifices a combination of:\n",
    "* `dataset_name`: A valid dataset name from `available_datasets`\n",
    "* `algorithm_name`: A valid dataset name from `available_algorithms`\n",
    "* `algorithm_params`: A dictionary of parameters to use when running the specified `algorithm`\n",
    "* `run_number`: (optional, default 1) A unique integer used to distinguish between different builds with otherwise identical parameters\n",
    "\n",
    "We don't need to know this, as we will use helper functions in `workflow` to build it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_model(dataset_name='lvq-pak_train',\n",
    "                   algorithm_name=\"linearSVC\",\n",
    "                   algorithm_params={'random_state': 42, 'max_iter': 200000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.get_model_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now running `make train` will train `LinearSVC` on `lvq-pak` with the specified parameters.\n",
    "\n",
    "Alternately, we can run `workflow.build_models()`.\n",
    "\n",
    "The output will be:\n",
    "* A trained model in `models/trained_models`\n",
    "* A json file `models/trained_models.json` that keeps track of the models that we've trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.build_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Caching! Then, checking against existing files and metadata and looking for caching! (note: will need a force parameter eventually)\n",
    "\n",
    "## TODO: Don't overwrite the trained_models.json, append to it (as long as the files are still there) --- add call to available_models in build_models and give it a force option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at the output from `make train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.paths import trained_model_path\n",
    "from src.utils import list_dir\n",
    "from src.utils import load_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the trained model\n",
    "from src.models.train import load_model\n",
    "\n",
    "tm, tm_metadata = load_model(model_name='linearSVC_lvq-pak_train_1', model_path=trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.load('lvq-pak_train')\n",
    "ds.DATA_HASH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: explore the effects of caching once it's implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any algorithm will work that: \n",
    "* is a subclass of the sklearn `BaseEstimator` class (needed for setting and getting params)\n",
    "* has a `fit` method (needed for `make train`)\n",
    "* has either a `predict` method (supervised) or a `transform` method (unsupervised) (needed for `make predict`)\n",
    "\n",
    "We will see how things work in the unsupervised case in the next example. \n",
    "\n",
    "Note that an **algorithm** here can be a combination of \"algorithms\" as long as that combination is a `BaseEstimator` with the above methods. For example, you can use an sklearn pipeline, or an sklearn meta estimator like GridSearchCV as an algorithm. \n",
    "\n",
    "If your algorithm of choice is **not yet** a `BaseEstimator` with the appropriate API, it is fairly easy to wrap it to be used in this way. While we won't have time to cover an example of this during the in-person part of this tutorial, the EDA Text Embedding (advanced usage tutorial) has an example of how to do this with gensim's FastText model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: `make predict`\n",
    "\n",
    "```\n",
    "## predict / transform / run experiments\n",
    "predict: models/predict_list.json\n",
    "\t$(PYTHON_INTERPRETER) -m src.models.predict_model predict_list.json\n",
    "```\n",
    "\n",
    "Similar to `models_list.json` in `predict_list.json` we specify the dataset to operate on, and in this case, the `trained_model` to apply to the given dataset. Again, we do this using the `workflow` module.\n",
    "\n",
    "\n",
    "A `predict_list.json` is a list of dicts, where each dict specifices a combination of:\n",
    "* `dataset_name`: A valid dataset name from `available_datasets`\n",
    "* `dataset_params`: A dictionary of parameters that can be passed to `load_dataset()` with the specified `dataset`\n",
    "* `model_name`: A valid dataset name from `available_trained_models` (aka. a key name in `trained_models.json`\n",
    "* `is_supervised`: Whether to use the `predict` (supervised) or `transform` (unsupervised) method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the test set here to do the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_prediction(dataset_name='lvq-pak_test', model_name='linearSVC_lvq-pak_train_1', is_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.get_prediction_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.run_predictions(predict_file='predict_list.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && LOGLEVEL=INFO make predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.paths import model_output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Predictions are just Datasets tagged with experiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ds = Dataset.load('linearSVC_lvq-pak_train_1_exp_lvq-pak_test_1', data_path=model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ds.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ds.metadata['experiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check that our prediction matches\n",
    "all(predict_ds.data == p_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: `make summary`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Add all of this to the standard workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_list = [{\n",
    "    'summarizer_name': 'supervised_score_df',\n",
    "    'summarizer_params': {}\n",
    "}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.paths import reports_path\n",
    "from src.utils import save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(reports_path / 'summary_list.json', summarizer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Outputs available via available_sumamries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Add caching of the summary dfs to know if you're about to overwrite one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Aside: Add other algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Add GradientBoostingClassifier and some other sklearn Classifier of your choice\n",
    "\n",
    "### Advanced Exercise: Use GridSearchCV applied to your classifier of choice as the 3rd alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_model(\n",
    "    dataset_name = 'lvq-pak_train',\n",
    "    algorithm_name = 'GradientBoostingClassifier',\n",
    "    algorithm_params = {'random_state': 42}    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add your choice of classifier here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Take a look to see what's there\n",
    "workflow.get_model_list()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Advanced example...see if you can make it work with this as well.\n",
    "\n",
    "workflow.add_model(\n",
    "    dataset_name = 'lvq-pak_train',\n",
    "    algorithm_name = 'GridSearchCV',\n",
    "    algorithm_params = {'alg_name': 'RandomForestClassifier',\n",
    "                             'alg_params': {'n_estimators': 200},\n",
    "                             'gridsearch_params':{'max_features':['sqrt', 'log2', 10],\n",
    "                                                   'max_depth':[5, 7, 9],\n",
    "                                                   'random_state':[42, 62345, 3457],\n",
    "                                                   },\n",
    "                             'params': {'cv': 3}\n",
    "                       }  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.get_model_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_algorithms(keys_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.get_prediction_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up predictions using all of the available models\n",
    "for tm in workflow.available_models():\n",
    "    workflow.add_prediction(\n",
    "        dataset_name = 'lvq-pak_test',\n",
    "        model_name = tm,\n",
    "        is_supervised = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.get_prediction_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && LOGLEVEL=DEBUG make predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default for running the the summary df is to run on all available predictions. We have nothing more that we have to add to our existing script to get all the new scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: add the next part to a `load_summary` call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.paths import summary_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dir(summary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_csv(summary_path / 'supervised_score_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: `make report`\n",
    "\n",
    "### Bonus Exercise: Turn your dataframe into a table that you can share with the Swedish Chef in a report that justifies why you picked the model that you gave to him\n",
    "\n",
    "This is mostly a creative writing exercise, so we'll skip it here...however, if you're way ahead of the crowd, take a stab at creating the necessary Makefile target and corresponding scripts to create your report!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: `make publish`\n",
    "\n",
    "For this task, publishing involves:\n",
    "* versioning and releasing the code that we used to create the delivered model\n",
    "* versioning and sharing the supporting report\n",
    "\n",
    "We'll cover how to do both of these briefly at the end of the tutorial time permitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Figure out where and how to include a \"lesson\" on random_state\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bus_number]",
   "language": "python",
   "name": "conda-env-bus_number-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
